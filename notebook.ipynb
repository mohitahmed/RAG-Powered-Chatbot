{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from torch import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "np.seterr(all=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the first 10,000 rows of the dataset\n",
    "df = load_dataset(\"corbt/all-recipes\", split=\"train[:100]\")\n",
    "\n",
    "# Convert the dataset to a Pandas DataFrame\n",
    "df = df.to_pandas()\n",
    "\n",
    "# Preview the first 5 rows\n",
    "print(df.head())\n",
    "\n",
    "# Extract titles from the 'input' column for the first 10,000 entries\n",
    "df[\"titles\"] = df[\"input\"].str.split(\"Ing\").str[0]\n",
    "\n",
    "# View the DataFrame with titles\n",
    "print(df[[\"titles\", \"input\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_recipe_to_qa(recipe_text):\n",
    "    # Split the text into ingredients and directions based on \"Ingredients:\" and \"Directions:\"\n",
    "    split_text = recipe_text.split(\"Directions:\")\n",
    "    ingredients = split_text[0].replace(\"Ingredients:\", \"\").strip()\n",
    "    directions = split_text[1].strip() if len(split_text) > 1 else \"\"\n",
    "    recipe_name = recipe_text.split(\"Ingredients:\")[0].strip()\n",
    "\n",
    "    # Create a list of Q&A pairs\n",
    "    qa_pairs = [\n",
    "        (\"What is the name of the recipe?\", recipe_name),\n",
    "        (f\"What are the ingredients for {recipe_name} ?\", ingredients),\n",
    "        (f\"What are the directions for {recipe_name} ?\", directions),\n",
    "    ]\n",
    "\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"qa_pairs\"] = df[\"input\"].apply(preprocess_recipe_to_qa)\n",
    "# df.drop(columns=[\"input\"], inplace=True)\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def get_embeddings(text):\n",
    "    # Tokenize the input text and move it to the GPU\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(\n",
    "        device\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Get the embeddings (usually the last hidden state)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for each Q&A pair\n",
    "qa_embeddings = []\n",
    "for qa_pair_list in tqdm(df[\"qa_pairs\"], desc=\"Generating Q&A Embeddings\"):\n",
    "    # Concatenate question and answer into a single string for each pair and get embeddings\n",
    "    # if df.index.get_loc(qa_pair_list.name) == 0:\n",
    "    qa_embedding_list = []\n",
    "    for question, answer in qa_pair_list:\n",
    "        qa_text = f\"Question: {question} Answer: {answer}\"\n",
    "        embedding = get_embeddings(qa_text)\n",
    "        qa_embedding_list.append(embedding)\n",
    "    qa_embeddings.append(qa_embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is already created and has the 'titles' column\n",
    "# Generate embeddings for the titles with a progress bar using GPU acceleration\n",
    "titles_list = df[\"titles\"].tolist()  # Extract titles from the DataFrame\n",
    "embeddings = [\n",
    "    get_embeddings(title)\n",
    "    for title in tqdm(titles_list, desc=\"Generating Title Embeddings\")\n",
    "]\n",
    "\n",
    "# Create a DataFrame to ingest it to the database\n",
    "embeddings_df = pd.DataFrame(\n",
    "    {\n",
    "        \"input\": df[\"qa_pairs\"],\n",
    "        \"titles\": titles_list,\n",
    "        \"qa_pairs\": df[\"qa_pairs\"],\n",
    "        \"embeddings\": embeddings,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the embeddings DataFrame\n",
    "print(embeddings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "\n",
    "def get_relevant_docs(user_query, dataframe, top_n=3):\n",
    "    query_embeddings = np.array(get_embeddings(user_query))\n",
    "\n",
    "    def cosine_similarity(embedding):\n",
    "        return float(\n",
    "            np.dot(query_embeddings, embedding)\n",
    "            / (np.linalg.norm(query_embeddings) * np.linalg.norm(embedding))\n",
    "        )\n",
    "\n",
    "    embeddings_df[\"similarity\"] = embeddings_df[\"embeddings\"].apply(\n",
    "        lambda x: cosine_similarity(np.array(x)[0])\n",
    "    )\n",
    "\n",
    "    relevant_docs = embeddings_df.nlargest(top_n, \"similarity\")[\"input\"].tolist()\n",
    "    print(relevant_docs)\n",
    "    sorted_embeddings_df = embeddings_df.sort_values(by=\"similarity\", ascending=False)\n",
    "\n",
    "    return relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rag_prompt(query, relevant_passage):\n",
    "    # Ensure all elements in relevant_passage are strings before joining\n",
    "    relevant_passage = \" \".join([str(passage) for passage in relevant_passage])\n",
    "    prompt = (\n",
    "        f\"You are a helpful and informative recipe chatbot that answers questions using text from the reference passage included below.\\n\\n \"\n",
    "        f\"Add some extra information to make your response more helpful and engaging. \\n\\n\"\n",
    "        f\"only anwer the questions with the topic of the recipes,ingredients, directions and cooking methods.\\n\\n \"\n",
    "        f\"Maintain a friendly and conversational tone. If the passage is irrelevant, feel free to ignore it.\\n\\n\"\n",
    "        f\"Give the answer in a markdown format.\\n\\n\"\n",
    "        f\"If the answer contains Ingrediens, give them in a unordered list with a title format.\\n\\n\"\n",
    "        f\"QUESTION: '{query}'\\n\"\n",
    "        f\"PASSAGE: '{relevant_passage}'\\n\\n\"\n",
    "        f\"ANSWER:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_response(user_prompt):\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    answer = model.generate_content(user_prompt)\n",
    "    return answer.text\n",
    "\n",
    "\n",
    "def generate_answer(query):\n",
    "    relevant_text = get_relevant_docs(query, embeddings_df)\n",
    "    prompt = make_rag_prompt(query, relevant_passage=relevant_text)\n",
    "    answer = generate_response(prompt)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = generate_answer(\"Can you tell me how to buy a car?\")\n",
    "display(Markdown(answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
